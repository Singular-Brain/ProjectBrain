{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjectBrain_torch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKJulP4gyZjo"
      },
      "source": [
        "**Final Results:**\n",
        "\n",
        "```\n",
        "Numpy CPU: 33.72 s \n",
        "Numpy GPU: 22.60 s \n",
        "Torch CPU: 24.91 s \n",
        "Torch GPU:  4.57 s\n",
        "CuPy GPU:   8.36 s\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv1hifZAx1B3"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from timeit import timeit\n",
        "\n",
        "TOTAL_TIME = 1 #s\n",
        "DT = 0.001\n",
        "POPULATION_SIZE = 1000\n",
        "BASE_CURRENT = 10000\n",
        "CONNECTION_CHANCE = 1\n",
        "REPEAT_TEST = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaO8XHS7aztS",
        "outputId": "65aee367-d40a-44c1-c8ea-fa3f9a0f76bd"
      },
      "source": [
        "if (torch.cuda.is_available()):\n",
        "    DEVICE = 'cuda'\n",
        "    workers = 1\n",
        "else:\n",
        "    DEVICE = 'cpu'\n",
        "    workers = 0\n",
        "print(f'Device is set to {DEVICE}\\nNumber of workers: {workers}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device is set to cuda\n",
            "Number of workers: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6ou8k9p2gUn"
      },
      "source": [
        "# Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj6xpncjNiNS"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Stimulus:\n",
        "    def __init__(self, dt, output, neurons):\n",
        "        self.output = output\n",
        "        self.neurons = neurons\n",
        "        self.dt = dt\n",
        "\n",
        "    def __call__(self, timestep):\n",
        "        return self.output(timestep * self.dt)\n",
        "\n",
        "\n",
        "class NeuronGroup:\n",
        "    def __init__(self, dt, population_size, connection_chance, total_time, stimuli = set(),\n",
        "    neuron_type = \"LIF\", **kwargs):\n",
        "        self.dt = dt\n",
        "        self.N = population_size\n",
        "        self.total_timepoints = int(total_time/dt)\n",
        "        self.kwargs = kwargs\n",
        "        self.connection_chance = connection_chance\n",
        "        self.base_current = kwargs.get('base_current', 1E-9)\n",
        "        self.u_thresh = kwargs.get('u_thresh', 35E-3)\n",
        "        self.u_rest = kwargs.get('u_rest', -63E-3)\n",
        "        self.refractory_timepoints = kwargs.get('tau_refractory', 0.002) / self.dt\n",
        "        self.excitatory_chance = kwargs.get('excitatory_chance',  0.8)\n",
        "        self.refractory = np.ones((self.N,1)) * self.refractory_timepoints\n",
        "        self.current = np.zeros((self.N,1))\n",
        "        self.potential = np.ones((self.N,1)) * self.u_rest\n",
        "        self.spike_train = np.zeros((self.N, self.total_timepoints), dtype= np.bool)\n",
        "        self.weights = np.random.rand(self.N,self.N)\n",
        "        np.fill_diagonal(self.weights, 0)\n",
        "        self.connections = (np.random.rand(*self.weights.shape) + self.connection_chance).astype(np.int)\n",
        "        self.excitatory_neurons = (np.random.rand(*self.weights.shape) + self.excitatory_chance).astype(np.int) * 2 -1 \n",
        "        self.AdjacencyMatrix = self.connections * self.excitatory_neurons * self.weights\n",
        "        self.stimuli = np.array(list(stimuli))\n",
        "        self.StimuliAdjacency = np.zeros((self.N, len(stimuli)), dtype=np.bool)\n",
        "        for i, stimulus in enumerate(self.stimuli):\n",
        "            self.StimuliAdjacency[stimulus.neurons, i] = True\n",
        "\n",
        "    def LIF(self):\n",
        "        \"\"\"\n",
        "        Leaky Integrate-and-Fire Neural Model\n",
        "        \"\"\"\n",
        "        Rm = self.kwargs.get(\"Rm\", 135E6)\n",
        "        Cm = self.kwargs.get(\"Cm\", 14.7E-12)\n",
        "        tau_m = Rm*Cm\n",
        "        exp_term = np.exp(-self.dt/tau_m)\n",
        "        u_base = (1-exp_term) * self.u_rest\n",
        "        return u_base + exp_term * self.potential + self.current*self.dt/Cm \n",
        "\n",
        "    def get_stimuli_current(self):\n",
        "        call_stimuli =  np.vectorize(lambda stim: stim(self.timepoint))\n",
        "        stimuli_output = call_stimuli(self.stimuli)\n",
        "        stimuli_current = (stimuli_output * self.StimuliAdjacency).sum(axis = 1)\n",
        "        return stimuli_current.reshape(self.N,1)\n",
        "    \n",
        "    def run(self):\n",
        "        for self.timepoint in range(self.total_timepoints):\n",
        "            ### LIF update\n",
        "            self.refractory +=1\n",
        "            self.potential = self.LIF()\n",
        "            ### Reset currents\n",
        "            self.current = np.zeros((self.N,1)) \n",
        "            ### Spikes \n",
        "            spikes = self.potential>self.u_thresh\n",
        "            self.spike_train[:,self.timepoint] = spikes.ravel()\n",
        "            self.refractory *= np.logical_not(spikes)\n",
        "            ### Transfer currents + external sources\n",
        "            new_currents = (spikes * self.AdjacencyMatrix).sum(axis = 0).reshape(self.N,1) * self.base_current\n",
        "            open_neurons = self.refractory >= self.refractory_timepoints\n",
        "            self.current += new_currents * open_neurons\n",
        "            self.current += self.get_stimuli_current() #instead of this we can use :\n",
        "            # self.current = np.sum(self.stimuli_adjancy * self.stimuli_current,axis = 1, keepdims = True) \n",
        "            \n",
        "\n",
        "    def _spike_train_repr(self, spike_train):\n",
        "        string = ''\n",
        "        for t in spike_train:\n",
        "            string += '|' if t else ' '\n",
        "        return string\n",
        "\n",
        "    def display_spikes(self):\n",
        "        spike_train_display = ' id\\n' + '=' * 5 + '╔' + '═' * self.total_timepoints + '╗\\n'\n",
        "        for i, spike_train in enumerate(self.spike_train):\n",
        "            spike_train_display += str(i) + ' ' * (5 - len(str(i))) \\\n",
        "            + '║' + self._spike_train_repr(spike_train) + '║\\n'  \n",
        "        spike_train_display +=' ' * 5 + '╚' + '═' * self.total_timepoints + '╝'\n",
        "        print(spike_train_display)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ca5JkL2xq-j"
      },
      "source": [
        "stimuli = {Stimulus(0.001, lambda t: 2E-9, [0,1]),\n",
        "           Stimulus(0.001, lambda t: 2E-9 * t, [2]),\n",
        "           Stimulus(0.001, lambda t: 2E-9 * np.sin(500*t), [3])}\n",
        "G = NeuronGroup(dt = DT, \n",
        "                population_size = POPULATION_SIZE, \n",
        "                connection_chance = CONNECTION_CHANCE,\n",
        "                total_time = TOTAL_TIME,\n",
        "                base_current = 2E-9,\n",
        "                stimuli = stimuli)\n",
        "\n",
        "vectorized_time = timeit('G.run()', number = REPEAT_TEST, globals=globals())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiJM2b-DxxmJ",
        "outputId": "07a43d50-5f5a-4b25-ee96-f22711b7f770"
      },
      "source": [
        "print(\"CPU numpy time: \", vectorized_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU numpy time:  22.391187097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KeiKrV3ygpH",
        "outputId": "815b8cfb-c6b4-48e5-db0e-85818a11d9b3"
      },
      "source": [
        "print(\"GPU numpy time: \", vectorized_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU numpy time:  22.391187097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L8oQbmn2iWs"
      },
      "source": [
        "# Torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D67Znm8Oigj"
      },
      "source": [
        "\n",
        "\n",
        "class Stimulus:\n",
        "    def __init__(self, dt, output, neurons):\n",
        "        self.output = output\n",
        "        self.neurons = neurons\n",
        "        self.dt = dt\n",
        "\n",
        "    def __call__(self, timestep):\n",
        "        return self.output(timestep * self.dt)\n",
        "\n",
        "\n",
        "class NeuronGroup:\n",
        "    def __init__(self, dt, population_size, connection_chance, total_time, stimuli = set(),\n",
        "    neuron_type = \"LIF\", **kwargs):\n",
        "        self.dt = dt\n",
        "        self.N = population_size\n",
        "        self.total_timepoints = int(total_time/dt)\n",
        "        self.stimuli = stimuli\n",
        "        self.kwargs = kwargs\n",
        "        self.connection_chance = connection_chance\n",
        "        self.base_current = kwargs.get('base_current', 1E-9)\n",
        "        self.u_thresh = kwargs.get('u_thresh', 35E-3)\n",
        "        self.u_rest = kwargs.get('u_rest', -63E-3)\n",
        "        self.refractory_timepoints = kwargs.get('tau_refractory', 0.002) / self.dt\n",
        "        self.excitatory_chance = kwargs.get('excitatory_chance',  0.8)\n",
        "        self.refractory = torch.ones((self.N,1)).to(DEVICE) * self.refractory_timepoints\n",
        "        self.current = torch.zeros((self.N,1)).to(DEVICE)\n",
        "        self.potential = torch.ones((self.N,1)).to(DEVICE) * self.u_rest\n",
        "        self.spike_train = torch.zeros((self.N, self.total_timepoints), dtype= torch.bool).to(DEVICE)\n",
        "        self.weights = np.random.rand(self.N,self.N)\n",
        "        np.fill_diagonal(self.weights, 0)\n",
        "        self.weights = torch.from_numpy(self.weights)\n",
        "        self.connections = (torch.rand(*self.weights.shape) + self.connection_chance).type(torch.int)\n",
        "        self.excitatory_neurons = (torch.rand(*self.weights.shape) + self.excitatory_chance).type(torch.int) * 2 -1\n",
        "        self.AdjacencyMatrix = self.connections * self.excitatory_neurons * self.weights\n",
        "        self.AdjacencyMatrix = self.AdjacencyMatrix.to(DEVICE)\n",
        "        self.stimuli = np.array(list(stimuli))\n",
        "        self.StimuliAdjacency = np.zeros((self.N, len(stimuli)), dtype=np.bool)\n",
        "        for i, stimulus in enumerate(self.stimuli):\n",
        "            self.StimuliAdjacency[stimulus.neurons, i] = True\n",
        "\n",
        "    def LIF(self):\n",
        "        \"\"\"\n",
        "        Leaky Integrate-and-Fire Neural Model\n",
        "        \"\"\"\n",
        "        Rm = self.kwargs.get(\"Rm\", 135E6)\n",
        "        Cm = self.kwargs.get(\"Cm\", 14.7E-12)\n",
        "        tau_m = Rm*Cm\n",
        "        exp_term = torch.exp(torch.tensor(-self.dt/tau_m))\n",
        "        u_base = (1-exp_term) * self.u_rest\n",
        "        new_potential = u_base + exp_term * self.potential + self.current*self.dt/Cm\n",
        "        return new_potential.to(DEVICE)\n",
        "\n",
        "    def get_stimuli_current(self):\n",
        "        call_stimuli =  np.vectorize(lambda stim: stim(self.timepoint))\n",
        "        stimuli_output = call_stimuli(self.stimuli)\n",
        "        stimuli_current = (stimuli_output * self.StimuliAdjacency).sum(axis = 1)\n",
        "        return torch.tensor(stimuli_current.reshape(self.N,1)).to(DEVICE)\n",
        "    \n",
        "    def run(self):\n",
        "        for self.timepoint in range(self.total_timepoints):\n",
        "            ### LIF update\n",
        "            self.refractory +=1\n",
        "            self.potential = self.LIF()\n",
        "            ### Reset currents\n",
        "            self.current = torch.zeros((self.N,1)).to(DEVICE)\n",
        "            ### Spikes \n",
        "            spikes = self.potential>self.u_thresh\n",
        "            self.spike_train[:,self.timepoint] = spikes.ravel()\n",
        "            self.refractory *= torch.logical_not(spikes).to(DEVICE)\n",
        "            ### Transfer currents + external sources\n",
        "            new_currents = (spikes * self.AdjacencyMatrix).sum(axis = 0).reshape(self.N,1) * self.base_current\n",
        "            open_neurons = self.refractory >= self.refractory_timepoints\n",
        "            self.current += new_currents * open_neurons\n",
        "            self.current += self.get_stimuli_current()\n",
        "\n",
        "    def _spike_train_repr(self, spike_train):\n",
        "        string = ''\n",
        "        for t in spike_train:\n",
        "            string += '|' if t else ' '\n",
        "        return string\n",
        "\n",
        "    def display_spikes(self):\n",
        "        spike_train_display = ' id\\n' + '=' * 5 + '╔' + '═' * self.total_timepoints + '╗\\n'\n",
        "        for i, spike_train in enumerate(self.spike_train):\n",
        "            spike_train_display += str(i) + ' ' * (5 - len(str(i))) \\\n",
        "            + '║' + self._spike_train_repr(spike_train) + '║\\n'  \n",
        "        spike_train_display +=' ' * 5 + '╚' + '═' * self.total_timepoints + '╝'\n",
        "        print(spike_train_display)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzH6_ygOzEYi"
      },
      "source": [
        "stimuli = {Stimulus(0.001, lambda t: 2E-9, [0,1]),\n",
        "           Stimulus(0.001, lambda t: 2E-9 * t, [2]),\n",
        "           Stimulus(0.001, lambda t: 2E-9 * np.sin(500*t), [3])}\n",
        "G = NeuronGroup(dt = DT, \n",
        "                population_size = POPULATION_SIZE, \n",
        "                connection_chance = CONNECTION_CHANCE,\n",
        "                total_time = TOTAL_TIME,\n",
        "                base_current = 2E-9,\n",
        "                stimuli = stimuli)\n",
        "\n",
        "vectorized_time = timeit('G.run()', number = REPEAT_TEST, globals=globals())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aw7wTfyzEYl",
        "outputId": "dc25bb6f-79c9-436c-a995-1bb4dc996bea"
      },
      "source": [
        "print(\"CPU torch time: \", vectorized_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU torch time:  5.146830141999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU00ehi0zEYj",
        "outputId": "736d7b5f-665e-4b88-b5d5-492edeb2dd64"
      },
      "source": [
        "print(\"GPU torch time: \", vectorized_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU torch time:  5.146830141999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQd-wkZvOWME"
      },
      "source": [
        "## Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7BQAiJXOXmo"
      },
      "source": [
        "class RFSTDP:\n",
        "    def __init__(self, NeuronGroup,\n",
        "                 interval_time = 0.001, # seconds\n",
        "                 pre_post_rate = 0.001,\n",
        "                 reward_pre_post_rate = 0.002,\n",
        "                 post_pre_rate = -0.001,\n",
        "                 reward_post_pre_rate = 0.001,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Reward-modulated Flat STDP \n",
        "        \"\"\"\n",
        "        self.NeuronGroup = NeuronGroup\n",
        "        self.AdjacencyMatrix = NeuronGroup.AdjacencyMatrix\n",
        "        self.interval_timepoints = int(interval_time / NeuronGroup.dt) #timepoints\n",
        "        self.total_timepoints = NeuronGroup.total_timepoints\n",
        "        self.spike_train = NeuronGroup.spike_train\n",
        "        self.reward_based = True\n",
        "        self.pre_post_rate = pre_post_rate\n",
        "        self.post_pre_rate = post_pre_rate\n",
        "        self.reward_pre_post_rate = reward_pre_post_rate\n",
        "        self.reward_post_pre_rate = reward_post_pre_rate\n",
        "\n",
        "    def __call__(self, reward):\n",
        "        spike_train = self.NeuronGroup.spike_train\n",
        "        padded_spike_train = torch.nn.functional.pad(spike_train,\n",
        "            (self.interval_timepoints, self.interval_timepoints, 0, 0),\n",
        "             mode='constant', value=0)\n",
        "        for i in range(self.total_timepoints + self.interval_timepoints):\n",
        "            section = padded_spike_train[:,i:i+self.interval_timepoints]\n",
        "            span = section.sum(axis = 1).type(torch.bool)\n",
        "            first = padded_spike_train[:,i]\n",
        "            last = padded_spike_train[:,i+self.interval_timepoints]\n",
        "            if reward:\n",
        "                self.AdjacencyMatrix += self.reward_pre_post_rate * (first * span * self.AdjacencyMatrix)\n",
        "                self.AdjacencyMatrix += self.reward_post_pre_rate * (span  * last * self.AdjacencyMatrix)\n",
        "            if not reward:\n",
        "                self.AdjacencyMatrix += self.pre_post_rate * (first * span * self.AdjacencyMatrix)\n",
        "                self.AdjacencyMatrix += self.post_pre_rate * (span  * last * self.AdjacencyMatrix)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hvoFO58XWI5",
        "outputId": "453bb7d4-bad9-435d-bf22-4b5eab9c97f7"
      },
      "source": [
        "stimuli = {Stimulus(0.001, lambda t: 2E-9, [0,1]),\n",
        "        #    Stimulus(0.001, lambda t: 2E-9 * t, [2]),\n",
        "        #    Stimulus(0.001, lambda t: 2E-9 * np.sin(500*t), [3])\n",
        "           }\n",
        "G = NeuronGroup(dt = 0.001, \n",
        "                population_size = 5, \n",
        "                connection_chance = CONNECTION_CHANCE,\n",
        "                total_time = 1,\n",
        "                base_current = 2E-9,\n",
        "                stimuli = stimuli,\n",
        "                tau_refractory = 0.002)\n",
        "G.run()\n",
        "print(G.AdjacencyMatrix)\n",
        "learning = RFSTDP(G)\n",
        "learning(True)\n",
        "print(G.AdjacencyMatrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.6871,  0.4502,  0.6704,  0.0838],\n",
            "        [-0.4723,  0.0000,  0.9621,  0.6461,  0.9022],\n",
            "        [ 0.8080,  0.9272,  0.0000, -0.3516,  0.5366],\n",
            "        [ 0.0464,  0.2355, -0.4373,  0.0000,  0.7221],\n",
            "        [ 0.2028,  0.7988,  0.5973,  0.4617,  0.0000]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "tensor([[ 0.0000, 13.7124,  1.9033,  2.8259,  0.3528],\n",
            "        [-9.4247,  0.0000,  4.0680,  2.7233,  3.7991],\n",
            "        [16.1236, 18.5036,  0.0000, -1.4821,  2.2597],\n",
            "        [ 0.9257,  4.7004, -1.8488,  0.0000,  3.0409],\n",
            "        [ 4.0464, 15.9403,  2.5253,  1.9462,  0.0000]], device='cuda:0',\n",
            "       dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ylXsHoEaF8e",
        "outputId": "438884c3-888b-4efa-82d4-87081a5b99da"
      },
      "source": [
        "G.display_spikes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " id\n",
            "=====╔════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
            "0    ║ |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||║\n",
            "1    ║ |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||║\n",
            "2    ║  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  ||| ║\n",
            "3    ║  ||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  ║\n",
            "4    ║  |  ||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  ||  |||  |║\n",
            "     ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╝\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_faVsz1jEuA",
        "outputId": "61ebfd85-8c3a-496b-e1fd-f19ddea4d524"
      },
      "source": [
        "G.run()\n",
        "G.display_spikes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " id\n",
            "=====╔════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
            "0    ║||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||║\n",
            "1    ║||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||║\n",
            "2    ║ |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||║\n",
            "3    ║|||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |║\n",
            "4    ║|  ||||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  |||||  ||║\n",
            "     ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╝\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9rSjuUahGDY"
      },
      "source": [
        "# CuPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E42YtNyIig2H",
        "outputId": "5d94dd40-d60e-4a73-acb4-8e41f6adaa99"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9q5uFqHhkBr",
        "outputId": "989c22af-384e-491b-ab79-989183f41c9d"
      },
      "source": [
        "!pip install cupy-cuda110"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cupy-cuda110\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/cc/6886c38a071138dd2bc52ddfe09a90b1fa0f8a3ee17522958e7074504d74/cupy_cuda110-8.6.0-cp37-cp37m-manylinux1_x86_64.whl (165.3MB)\n",
            "\u001b[K     |████████████████████████████████| 165.3MB 35kB/s \n",
            "\u001b[?25hRequirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda110) (0.6)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda110) (1.19.5)\n",
            "Installing collected packages: cupy-cuda110\n",
            "Successfully installed cupy-cuda110-8.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BG72lothGDc"
      },
      "source": [
        "import cupy as cp\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Stimulus:\n",
        "    def __init__(self, dt, output, neurons):\n",
        "        self.output = output\n",
        "        self.neurons = neurons\n",
        "        self.dt = dt\n",
        "\n",
        "    def __call__(self, timestep):\n",
        "        return self.output(timestep * self.dt)\n",
        "\n",
        "\n",
        "class NeuronGroup:\n",
        "    def __init__(self, dt, population_size, connection_chance, total_time, stimuli = set(),\n",
        "    neuron_type = \"LIF\", **kwargs):\n",
        "        self.dt = dt\n",
        "        self.N = population_size\n",
        "        self.total_timepoints = int(total_time/dt)\n",
        "        self.kwargs = kwargs\n",
        "        self.connection_chance = connection_chance\n",
        "        self.base_current = kwargs.get('base_current', 1E-9)\n",
        "        self.u_thresh = kwargs.get('u_thresh', 35E-3)\n",
        "        self.u_rest = kwargs.get('u_rest', -63E-3)\n",
        "        self.refractory_timepoints = kwargs.get('tau_refractory', 0.002) / self.dt\n",
        "        self.excitatory_chance = kwargs.get('excitatory_chance',  0.8)\n",
        "        self.refractory = cp.ones((self.N,1), dtype = cp.uint8) * self.refractory_timepoints\n",
        "        self.current = cp.zeros((self.N,1), dtype = cp.float64)\n",
        "        self.potential = cp.ones((self.N,1), dtype = cp.float64) * self.u_rest\n",
        "        self.spike_train = cp.zeros((self.N, self.total_timepoints), dtype= cp.bool)\n",
        "        self.weights = cp.random.rand(self.N,self.N)\n",
        "        cp.fill_diagonal(self.weights, 0)\n",
        "        self.connections = (cp.random.rand(*self.weights.shape) + self.connection_chance).astype(cp.int)\n",
        "        self.excitatory_neurons = (cp.random.rand(*self.weights.shape) + self.excitatory_chance).astype(cp.int) * 2 -1 \n",
        "        self.AdjacencyMatrix = self.connections * self.excitatory_neurons * self.weights\n",
        "        self.stimuli = np.array(list(stimuli))\n",
        "        self.StimuliAdjacency = np.zeros((self.N, len(stimuli)), dtype=np.bool)\n",
        "        for i, stimulus in enumerate(self.stimuli):\n",
        "            self.StimuliAdjacency[stimulus.neurons, i] = True\n",
        "\n",
        "    def LIF(self):\n",
        "        \"\"\"\n",
        "        Leaky Integrate-and-Fire Neural Model\n",
        "        \"\"\"\n",
        "        Rm = self.kwargs.get(\"Rm\", 135E6)\n",
        "        Cm = self.kwargs.get(\"Cm\", 14.7E-12)\n",
        "        tau_m = Rm*Cm\n",
        "        exp_term = cp.exp(-self.dt/tau_m)\n",
        "        u_base = (1-exp_term) * self.u_rest\n",
        "        return u_base + exp_term * self.potential + self.current*self.dt/Cm \n",
        "\n",
        "    def get_stimuli_current(self):\n",
        "        call_stimuli =  np.vectorize(lambda stim: stim(self.timepoint))\n",
        "        stimuli_output = call_stimuli(self.stimuli)\n",
        "        stimuli_current = (stimuli_output * self.StimuliAdjacency).sum(axis = 1)\n",
        "        return cp.asarray(stimuli_current.reshape(self.N,1))\n",
        "    \n",
        "    def run(self):\n",
        "        for self.timepoint in range(self.total_timepoints):\n",
        "            ### LIF update\n",
        "            self.refractory +=1\n",
        "            self.potential = self.LIF()\n",
        "            ### Reset currents\n",
        "            self.current = cp.zeros((self.N,1)) \n",
        "            ### Spikes \n",
        "            spikes = self.potential>self.u_thresh\n",
        "            self.spike_train[:,self.timepoint] = spikes.ravel()\n",
        "            self.refractory *= cp.logical_not(spikes)\n",
        "            ### Transfer currents + external sources\n",
        "            new_currents = (spikes * self.AdjacencyMatrix).sum(axis = 0).reshape(self.N,1) * self.base_current\n",
        "            open_neurons = self.refractory >= self.refractory_timepoints\n",
        "            self.current += new_currents * open_neurons\n",
        "            self.current += self.get_stimuli_current() #instead of this we can use :\n",
        "            # self.current = cp.sum(self.stimuli_adjancy * self.stimuli_current,axis = 1, keepdims = True) \n",
        "            \n",
        "\n",
        "    def _spike_train_repr(self, spike_train):\n",
        "        string = ''\n",
        "        for t in spike_train:\n",
        "            string += '|' if t else ' '\n",
        "        return string\n",
        "\n",
        "    def display_spikes(self):\n",
        "        spike_train_display = ' id\\n' + '=' * 5 + '╔' + '═' * self.total_timepoints + '╗\\n'\n",
        "        for i, spike_train in enumerate(self.spike_train):\n",
        "            spike_train_display += str(i) + ' ' * (5 - len(str(i))) \\\n",
        "            + '║' + self._spike_train_repr(spike_train) + '║\\n'  \n",
        "        spike_train_display +=' ' * 5 + '╚' + '═' * self.total_timepoints + '╝'\n",
        "        print(spike_train_display)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqpFaTlbhGDe"
      },
      "source": [
        "stimuli = {Stimulus(0.001, lambda t: 2E-9, [0,1]),\n",
        "           Stimulus(0.001, lambda t: 2E-9 * t, [2]),\n",
        "           Stimulus(0.001, lambda t: 2E-9 * cp.sin(500*t), [3])}\n",
        "G = NeuronGroup(dt = DT, \n",
        "                population_size = POPULATION_SIZE, \n",
        "                connection_chance = CONNECTION_CHANCE,\n",
        "                total_time = TOTAL_TIME,\n",
        "                base_current = 2E-9,\n",
        "                stimuli = stimuli)\n",
        "\n",
        "vectorized_time = timeit('G.run()', number = REPEAT_TEST, globals=globals())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA_FiT_0hGDf",
        "outputId": "3538bcb4-350e-4c91-d732-2e1f7d0b61ca"
      },
      "source": [
        "print(\"GPU CuPy time: \", vectorized_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU CuPy time:  9.690362012000008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q32QvlmglCPD"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}